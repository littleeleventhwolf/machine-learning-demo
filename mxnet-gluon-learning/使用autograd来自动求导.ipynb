{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用autograd来自动求导\n",
    "\n",
    "在机器学习中，我们通常使用**梯度下降(gradient descent)**来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。虽然梯度计算比较直观，但对于复杂的模型，例如多达数十层的神经网络，手动计算梯度非常困难。\n",
    "\n",
    "为此MXNet提供autograd包来自动化求导过程。虽然大部分的深度学习框架要求编译计算图来自动求导，mxnet.autograd可以对正常的命令式程序进行求导，它每次在后端实时创建计算图，从而可以立即得到梯度的计算方法。\n",
    "\n",
    "下面让我们一步步介绍这个包。我们先导入autograd。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "import mxnet.autograd as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为变量附上梯度\n",
    "\n",
    "假设我们想对函数$f = 2 \\times x^2$求关于x的导数。我们先创建变量x，并赋初值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nd.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当进行求导的时候，我们需要一个地方来存x的导数，这个可以通过NDArray的方法attach_grad()来要求系统申请对应的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义f。默认条件下，MXNet不会自动记录和构建用于求导的计算图，我们需要使用autograd里的record()函数来显式的要求MXNet记录我们需要求导的程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ag.record():\n",
    "    y = x * 2\n",
    "    z = y * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们可以通过z.backward()来进行求导。如果z不是一个标量，那么z.backward()等价于nd.sum(z).backward()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们来看求出来的导数是不是正确的。注意到y = x \\* 2和z = x \\* y，所以z等价于2 \\* x \\* x。它的导数那么就是$\\frac{dz}{dx} = 4 \\times x$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x.grad: ', \n",
      "[[  4.   8.]\n",
      " [ 12.  16.]]\n",
      "<NDArray 2x2 @cpu(0)>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  1.]\n",
       " [ 1.  1.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('x.grad: ', x.grad)\n",
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对控制流求导\n",
    "\n",
    "命令式的编程的一个便利之处是几乎可以对任意的可导程序进行求导，即使里面包含了Python的控制流。考虑下面的程序，里面包含控制流for和if，但循环迭代的次数和判断语句的执行都是取决于输入的值。不同的输入会导致这个程序的执行不一样。（对于计算图框架来说，这个对应于动态图，就是图的结构会根据输入数据不同而改变）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while nd.norm(b).asscalar() < 1000:\n",
    "        b = b * 2\n",
    "    if nd.sum(b).asscalar() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以跟之前一样使用record记录和backward求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.random_normal(shape=3)\n",
    "a.attach_grad()\n",
    "with ag.record():\n",
    "    c = f(a)\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到给定输入a，其输出$f(a) = x \\times a$，x的值取决于输入a。所以有$\\frac{df}{da} = x$，我们可以很简单地评估自动求导的导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.  1.  1.]\n",
       "<NDArray 3 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == c / a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 头梯度和链式法则\n",
    "\n",
    "**注意：读者可以跳过这一小节，不会影响阅读之后的章节**\n",
    "\n",
    "当我们在一个NDArray上调用backward方法时，例如y.backward()，此处y是一个关于x的函数，我们将求得y关于x的导数。数学家们会把这个求导写成$\\frac{dy(x)}{dx}$。还有些更复杂的情况，比如z是关于y的函数，且y是关于x的函数，我们想对z关于x求导，也就是求$\\frac{d}{dx}z(y(x))$的结果。回想一下链式法则，我们可以得到$\\frac{d}{dx}z(y(x)) = \\frac{dz(y)}{dy}\\frac{dy(x)}{dx}$。当y是一个更大的z函数的一部分，并且我们希望求得$\\frac{dz}{dx}$保存在x.grad中时，我们可以传入**头梯度(head gradient)**$\\frac{dz}{dy}$的值作为backward()方法的输入参数，系统会自动应用链式法则进行计算。这个参数的默认值是nd.ones_like(y)。关于链式法则的详细解释，请参阅[Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 40.           8.        ]\n",
      " [  1.20000005   0.16      ]]\n",
      "<NDArray 2x2 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "with ag.record():\n",
    "    y = x * 2\n",
    "    z = y * x\n",
    "\n",
    "head_gradient = nd.array([[10, 1.], [.1, .01]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
