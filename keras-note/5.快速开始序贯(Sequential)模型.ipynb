{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速开始序贯（Sequential）模型\n",
    "\n",
    "序贯模型是多个网络层的线性堆叠，也就是“一条路走到黑”。\n",
    "\n",
    "可以通过向Sequential模型传递一个layer的list来构造该模型：\n",
    "\n",
    "``` python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, units=784),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "```\n",
    "\n",
    "也可以通过.add()方法一个个的将layer加入模型中：\n",
    "``` python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784, )))\n",
    "model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "## 指定输入数据的shape\n",
    "\n",
    "模型需要知道输入数据的shape，因此，Sequential的第一层需要接受一个关于输入数据shape的参数，后面的各个层则可以自动的推导出中间数据的shape，因此不需要为每个层都指定这个参数。有几种方法来为第一层指定输入数据的shape：\n",
    "- 传递一个input_shape的关键字参数给第一层，input_shape是一个tuple类型的数据，其中也可以填入None，如果填入None则表示此位置可能是任何正整数。数据的batch大小不应该包含在其中。\n",
    "- 有些2D层，如Dense，支持通过指定其输入维度input_dim来隐含的指定输入数据shape，是一个Int类型的数据。一些3D的域层支持通过参数input_dim和input_length来指定输入shape。\n",
    "- 如果你需要为输入指定一个固定大小的batch_size（常常用于stateful RNN网络），可以传递batch_size参数到一个层中，例如你想指定输入张量的batch的大小是32，数据shape是(6, 8)，则你需要传递batch_size=32和input_shape=(6, 8)\n",
    "\n",
    "``` python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))\n",
    "```\n",
    "\n",
    "``` python\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784, )))\n",
    "```\n",
    "\n",
    "## 编译\n",
    "\n",
    "在训练模型之前，我们需要通过compile来对学习过程进行配置。compile接收三个参数：\n",
    "- 优化器optimizer：该参数可指定为已预定义的优化器名，如rmsprop、adagrad，或一个Optimizer类的对象，详情见[optimizers](https://keras-cn.readthedocs.io/en/latest/other/optimizers/)。\n",
    "- 损失函数loss：该参数为模型试图最小化的目标函数，它可为预定义的损失函数名，如categorical_crossentropy、mse，也可以为一个损失函数。详情见[losses](https://keras-cn.readthedocs.io/en/latest/other/objectives/)。\n",
    "- 指标列表metrics：对分类问题，我们一般将该列表设置为metrics=\\['accuracy'\\]。指标可以是一个预定义指标的名字，也可以是一个用户定制的函数。指标函数应该返回单个张量，或一个完成metric_name->metric_value映射的字典。请参考[性能评估](https://keras-cn.readthedocs.io/en/latest/getting_started/other/metrices.md)。\n",
    "\n",
    "``` python\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a binary classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# For a mean squared error regression problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mse')\n",
    "\n",
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])\n",
    "```\n",
    "\n",
    "## 训练\n",
    "\n",
    "Keras以Numpy数组作为输入数据和标签的数据类型。训练模型一般使用fit函数，该函数的详情见[这里](https://keras-cn.readthedocs.io/en/latest/models/sequential/)。下面是一些例子。\n",
    "``` python\n",
    "# For a single-input model with 2 classes (binary classification):\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "```\n",
    "<hr>\n",
    "``` python\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "## 例子\n",
    "\n",
    "这里是一些帮助你开始的例子。\n",
    "\n",
    "在Keras代码包的examples文件夹中，你将找到使用真实数据的示例模型：\n",
    "- CIFAR10小图片分类：使用CNN和实时数据提升\n",
    "- IMDB电影评论观点分类：使用LSTM处理成序列的词语\n",
    "- Reuters(路透社)新闻主题分类：使用多层感知器（MLP）\n",
    "- MNIST手写数字识别：使用多层感知器和CNN\n",
    "- 字符级文本生成：使用LSTM\n",
    "\n",
    "<font color=#0099ff size=4 face=\"黑体\">基于多层感知器的softmax分类</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 88us/step - loss: 2.4432 - acc: 0.0990\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.3545 - acc: 0.1100\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3441 - acc: 0.0990\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3280 - acc: 0.0980\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3075 - acc: 0.1140\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3148 - acc: 0.1050\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3141 - acc: 0.1060\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3165 - acc: 0.1110\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3142 - acc: 0.1070\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.3130 - acc: 0.0930\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3076 - acc: 0.1000\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.3093 - acc: 0.1060\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3138 - acc: 0.1050\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3088 - acc: 0.0980\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.2983 - acc: 0.1110\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.2953 - acc: 0.1310\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3069 - acc: 0.1130\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.2977 - acc: 0.1040\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.3026 - acc: 0.1160\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.2958 - acc: 0.1320\n",
      "100/100 [==============================] - 0s 178us/step\n",
      "[2.3034238815307617, 0.14000000059604645]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">MLP的二分类</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 158us/step - loss: 0.7504 - acc: 0.4820\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.7154 - acc: 0.5010\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.7191 - acc: 0.4870\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.7083 - acc: 0.5090\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.7076 - acc: 0.4830\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.7078 - acc: 0.4860\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.7035 - acc: 0.4970\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6991 - acc: 0.5320\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6968 - acc: 0.5100\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6933 - acc: 0.5190\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6920 - acc: 0.5320\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 0.6986 - acc: 0.5050\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6971 - acc: 0.4850\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6980 - acc: 0.5070\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 13us/step - loss: 0.6984 - acc: 0.4950\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6939 - acc: 0.5210\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6898 - acc: 0.5220\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 0.6924 - acc: 0.5020\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 0.6889 - acc: 0.5190\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 0.6890 - acc: 0.5370\n",
      "100/100 [==============================] - 0s 259us/step\n",
      "test_loss: 0.698766648769 \n",
      "test_accuracy: 0.469999998808\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test_loss:', score[0], '\\ntest_accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">类似VGG的卷积神经网络</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 3s 34ms/step - loss: 2.3092\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.2954\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.2777\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.3074\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.2773\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 2.2819\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 2.2649\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 2.2653\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 2.2873\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 2.2877\n",
      "20/20 [==============================] - 0s 12ms/step\n",
      "2.29371023178\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-06, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">使用LSTM的序列分类</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 834s 48us/step\n",
      "17473536/17464789 [==============================] - 834s 48us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x times)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4596 - acc: 0.7820 - val_loss: 0.3853 - val_acc: 0.8338\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.2955 - acc: 0.8801 - val_loss: 0.3709 - val_acc: 0.8372\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.2197 - acc: 0.9138 - val_loss: 0.4337 - val_acc: 0.8244\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1523 - acc: 0.9421 - val_loss: 0.4896 - val_acc: 0.8307\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.1083 - acc: 0.9608 - val_loss: 0.5839 - val_acc: 0.8221\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.0779 - acc: 0.9726 - val_loss: 0.6623 - val_acc: 0.8172\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 87s 3ms/step - loss: 0.0575 - acc: 0.9798 - val_loss: 0.7046 - val_acc: 0.8196\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0518 - acc: 0.9825 - val_loss: 0.8519 - val_acc: 0.8186\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0321 - acc: 0.9891 - val_loss: 0.8965 - val_acc: 0.8109\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0267 - acc: 0.9914 - val_loss: 1.0198 - val_acc: 0.8149\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0228 - acc: 0.9928 - val_loss: 0.9757 - val_acc: 0.8134\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0145 - acc: 0.9957 - val_loss: 1.0283 - val_acc: 0.7999\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.0129 - acc: 0.9958 - val_loss: 1.1145 - val_acc: 0.8118\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 1.2982 - val_acc: 0.8085\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.0087 - acc: 0.9972 - val_loss: 1.1377 - val_acc: 0.8080\n",
      "25000/25000 [==============================] - 14s 550us/step\n",
      "Test score: 1.13773898368\n",
      "Test accuracy: 0.80796\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80 # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size=32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x times)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=15, \n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">使用1D卷积的序列分类</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 114s 5ms/step - loss: 0.4056 - acc: 0.7994 - val_loss: 0.3166 - val_acc: 0.8618\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 114s 5ms/step - loss: 0.2287 - acc: 0.9076 - val_loss: 0.2922 - val_acc: 0.8774\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.1668 - acc: 0.9373 - val_loss: 0.2692 - val_acc: 0.8937\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.1142 - acc: 0.9580 - val_loss: 0.3061 - val_acc: 0.8905\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0761 - acc: 0.9724 - val_loss: 0.3577 - val_acc: 0.8866\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0567 - acc: 0.9794 - val_loss: 0.4035 - val_acc: 0.8843\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0475 - acc: 0.9825 - val_loss: 0.4042 - val_acc: 0.8852\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0349 - acc: 0.9878 - val_loss: 0.4999 - val_acc: 0.8741\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0428 - acc: 0.9841 - val_loss: 0.4709 - val_acc: 0.8844\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 116s 5ms/step - loss: 0.0306 - acc: 0.9890 - val_loss: 0.5495 - val_acc: 0.8825\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0284 - acc: 0.9894 - val_loss: 0.5691 - val_acc: 0.8724\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0236 - acc: 0.9915 - val_loss: 0.5692 - val_acc: 0.8828\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0261 - acc: 0.9903 - val_loss: 0.6017 - val_acc: 0.8778\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0239 - acc: 0.9906 - val_loss: 0.5493 - val_acc: 0.8836\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 115s 5ms/step - loss: 0.0198 - acc: 0.9928 - val_loss: 0.5582 - val_acc: 0.8813\n",
      "25000/25000 [==============================] - 18s 719us/step\n",
      "Test loss: 0.558236720802\n",
      "Test accuracy: 0.88128\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 15\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', \n",
    "                 activation='relu', strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# we add a vanilla hidden layer\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# we project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, \n",
    "          epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">用于序列分类的栈式LSTM</font>\n",
    "\n",
    "在该模型中，我们将三个LSTM堆叠在一起，使该模型能够学习更高层次的时域特征表示。\n",
    "\n",
    "开始的两层LSTM返回其全部输出序列，而第三层LSTM只返回其输出序列的最后一步结果，从而其时域维度降低（即将输入序列转换为单个向量）。\n",
    "\n",
    "![栈式LSTM](./5.attaches/regular_stacked_lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 11.6169 - acc: 0.1020 - val_loss: 11.5961 - val_acc: 0.0700\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 276us/step - loss: 11.6144 - acc: 0.1060 - val_loss: 11.5923 - val_acc: 0.1000\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 284us/step - loss: 11.6144 - acc: 0.0960 - val_loss: 11.5923 - val_acc: 0.1200\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 275us/step - loss: 11.6143 - acc: 0.1010 - val_loss: 11.5929 - val_acc: 0.0700\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 275us/step - loss: 11.6135 - acc: 0.1040 - val_loss: 11.5941 - val_acc: 0.1100\n",
      "100/100 [==============================] - 0s 126us/step\n",
      "Test loss: 11.5940558624\n",
      "Test accuracy: 0.11\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "# returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "# returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "# returns a single vector of dimension 32\n",
    "model.add(LSTM(32))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((100, timesteps, data_dim))\n",
    "y_val = np.random.random((100, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#0099ff size=4 face=\"黑体\">采用stateful LSTM的相同模型</font>\n",
    "\n",
    "stateful LSTM的特点是，在处理过一个batch的训练数据后，其内部状态（记忆）会被作为下一个batch的训练数据的初始状态。状态LSTM使得我们可以在合理的计算复杂度内处理较长序列。\n",
    "\n",
    "请在FAQ中关于[stateful LSTM](https://keras-cn.readthedocs.io/en/latest/for_beginners/FAQ/)的部分获取更多信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320 samples, validate on 96 samples\n",
      "Epoch 1/5\n",
      "320/320 [==============================] - 2s 7ms/step - loss: 11.4301 - acc: 0.1313 - val_loss: 11.4971 - val_acc: 0.0521\n",
      "Epoch 2/5\n",
      "320/320 [==============================] - 0s 242us/step - loss: 11.4246 - acc: 0.1219 - val_loss: 11.4976 - val_acc: 0.0521\n",
      "Epoch 3/5\n",
      "320/320 [==============================] - 0s 250us/step - loss: 11.4233 - acc: 0.1187 - val_loss: 11.4980 - val_acc: 0.0521\n",
      "Epoch 4/5\n",
      "320/320 [==============================] - 0s 236us/step - loss: 11.4223 - acc: 0.1156 - val_loss: 11.4985 - val_acc: 0.0521\n",
      "Epoch 5/5\n",
      "320/320 [==============================] - 0s 254us/step - loss: 11.4213 - acc: 0.1250 - val_loss: 11.4992 - val_acc: 0.0521\n",
      "96/96 [==============================] - 0s 129us/step\n",
      "Test loss: 11.5005394618\n",
      "Test accuracy: 0.0625\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "data_dim = 16\n",
    "timesteps = 8\n",
    "num_classes = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True, \n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, \n",
    "          shuffle=False, validation_data=(x_val, y_val))\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=batch_size)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
