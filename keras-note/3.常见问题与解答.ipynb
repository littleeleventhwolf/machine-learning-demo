{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见问题与解答\n",
    "\n",
    "## 如何引用Keras？\n",
    "\n",
    "如果Keras对你的研究有帮助的话，请在你的文章中引用Keras。这里是一个使用BibTex的例子：\n",
    "```\n",
    "@misc{chollet2015keras,\n",
    "  author = {Chollet, François and others},\n",
    "  title = {Keras},\n",
    "  year = {2015},\n",
    "  publisher = {GitHub},\n",
    "  journal = {GitHub repository},\n",
    "  howpublished = {\\url{https://github.com/fchollet/keras}}\n",
    "}\n",
    "```\n",
    "\n",
    "## 如何使Keras调用GPU？\n",
    "\n",
    "如果采用Tensorflow作为后端，当机器上有可用的GPU时，代码会自动调用GPU进行并行计算。如果使用Theano作为后端，可以通过以下方法设置：\n",
    "\n",
    "方法1：使用Theano标记\n",
    "\n",
    "在执行Python脚本时使用下面的命令：\n",
    "``` shell\n",
    "THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py\n",
    "```\n",
    "\n",
    "方法2：设置.theanorc文件\n",
    "\n",
    "点击[这里](http://deeplearning.net/software/theano/library/config.html)查看指导教程。\n",
    "\n",
    "方法3：在代码的开头手动设置theano.config.device和theano.config.floatX：\n",
    "\n",
    "``` python\n",
    "import theano\n",
    "theano.config.device = 'gpu'\n",
    "theano.config.floatX = 'float32'\n",
    "```\n",
    "\n",
    "## 如何在多张GPU卡上使用Keras？\n",
    "\n",
    "我们建议有多张GPU卡可用时，使用Tensorflow后端。\n",
    "\n",
    "有两种方法可以在多张GPU上运行一个模型：数据并行/设备并行。\n",
    "\n",
    "大多数情况下，你需要的很可能是“数据并行”。\n",
    "\n",
    "### 数据并行\n",
    "\n",
    "数据并行将目标模型在多个设备上各复制一份，并使用每个设备上的复制品处理整个数据集的不同部分数据。Keras在keras.util.multi_gpu_model中提供有内置函数，该函数可以产生任意模型的数据并行版本，最高支持在8片GPU上并行。下面是一个例子：\n",
    "``` python\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "# Replicates `model` on 8 GPUs.\n",
    "# This assumes that your machine has 8 available GPUs.\n",
    "parallel_model = multi_gpu_model(model, gpus=8)\n",
    "parallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# This `fit` call will be distributed on 8 GPUs.\n",
    "# Since the batch size is 256, each GPU will process 32 samples.\n",
    "parallel_model.fit(x, y, epochs=20, batch_size=256)\n",
    "```\n",
    "\n",
    "### 设备并行\n",
    "\n",
    "设备并行是在不同设备上运行同一个模型的不同部分，当模型含有多个并行结构，例如含有两个分支时，这种方式很适合。\n",
    "\n",
    "这种并行方法可以通过使用Tensorflow device scopes实现，下面是一个例子：\n",
    "``` python\n",
    "# Model where a shared LSTM is used to encode two different sequences in parallel.\n",
    "tweet_a = keras.Input(shape=(140, 256))\n",
    "tweet_b = keras.Input(shape=(140, 256))\n",
    "\n",
    "shared_lstm = keras.layers.LSTM(64)\n",
    "\n",
    "# Process the first sequence on one GPU\n",
    "with tf.device_scope('/gpu:0'):\n",
    "    encoded_a = shared_lstm(tweet_a)\n",
    "# Process the next sequence on another GPU\n",
    "with tf.device_scope('/gpu:1'):\n",
    "    encoded_b = shared_lstm(tweet_b)\n",
    "\n",
    "# Concatenate results on CPU\n",
    "with tf.device_scope('/cpu:0'):\n",
    "    merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n",
    "```\n",
    "\n",
    "## \"batch\"，\"epoch\"和\"sample\"都是什么意思？\n",
    "\n",
    "下面是一些使用Keras时常会遇到的概念，我们来简单解释。\n",
    "- sample：样本，数据集中的一条数据。例如图片数据集中的一张图片，语音数据中的一段音频。\n",
    "- batch：中文为批，一个batch由若干条数据构成。batch是进行网络优化的基本单位，网络参数的每一轮优化需要使用一个batch。batch中的样本是被并行处理的。与单个样本相比，一个batch的数据能更好的模拟数据集的分布，batch越大则对输入数据分布模拟的越好，反映在网络训练上，则体现为能让网络训练的方向“更加正确”。但另一方面，一个batch也只能让网络的参数更新一次，因此网络参数的迭代会较慢。在测试网络的时候，应该在条件的允许范围内尽量使用更大的batch，这样计算效率会更高。\n",
    "- epoch：epoch可译为“轮次”。如果说每个batch对应网络的一次更新的话，一个epoch对应的就是网络的一轮更新。每一轮更新中网络的更新次数可以随意，但通常会设置为遍历一遍数据集。因此一个epoch的含义是模型完整的看了一遍数据集。设置epoch的主要作用是把模型的训练的整个训练过程分为若干段，这样我们可以更好的观察和调整模型的训练。Keras中，当指定了验证集时，每个epoch执行完后都会运行一次验证集以确定模型的性能。另外，我们可以使用回调函数在每个epoch的训练前后执行一些操作，如调整学习率，打印目前模型的一些信息等，详情请参考callback一节。\n",
    "\n",
    "## 如何保存Keras模型？\n",
    "\n",
    "我们不推荐使用pickle或cPickle来保存Keras模型。\n",
    "\n",
    "你可以使用model.save(filepath)将Keras模型和权重保存在一个HDF5文件中，该文件将包含：\n",
    "- 模型的结构，以便重构该模型\n",
    "- 模型的权重\n",
    "- 训练配置（损失函数，优化器等）\n",
    "- 优化器的状态，以便于从上次训练中断的地方开始\n",
    "\n",
    "使用keras.models.load_model(filepath)来重新实例化你的模型，如果文件中存储了训练配置的话，该函数还会同时完成模型的编译。\n",
    "\n",
    "例子：\n",
    "``` python\n",
    "from keras.models.import load_model\n",
    "\n",
    "model.save('my_model.h5') # creates a HDF5 file 'my_model.h5'\n",
    "del model # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('my_model.h5')\n",
    "```\n",
    "\n",
    "如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：\n",
    "``` python\n",
    "# save as JSON\n",
    "json_string = model.to_json()\n",
    "\n",
    "# save as YAML\n",
    "yaml_string = model.to_yaml()\n",
    "```\n",
    "\n",
    "这项操作将模型序列化为json或yaml文件，这些文件对人而言也是友好的，如果需要的话你甚至可以手动打开这些文件并进行编辑。\n",
    "\n",
    "当然，你也可以从保存好的json文件或yaml文件中载入模型：\n",
    "``` python\n",
    "from keras.model import model_from_json, model_from_yaml\n",
    "# model reconstruction from JSON:\n",
    "model = model_from_json(json_string)\n",
    "# model reconstruction from YAML:\n",
    "model = model_from_yaml(yaml_string)\n",
    "```\n",
    "\n",
    "如果需要保存模型的权重，可通过下面的代码利用HDF5进行保存。注意，在使用前需要确保你已安装了HDF5和其Python库h5py：\n",
    "``` python\n",
    "model.save_weights('my_model_weights.h5')\n",
    "```\n",
    "\n",
    "如果你需要在代码中初始化一个完全相同的模型，请使用：\n",
    "``` python\n",
    "model.load_weights('my_model_weights.h5')\n",
    "```\n",
    "\n",
    "如果你需要加载权重到不同的网络结构（有些层一样）中，例如file-tune或transfer-learning，你可以通过层名字来加载模型：\n",
    "``` python\n",
    "model.load_weights('my_model_weights.h5', by_name=True)\n",
    "```\n",
    "\n",
    "例如：\n",
    "``` python\n",
    "\"\"\"\n",
    "假如原模型为：\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=3, name=\"dense_1\"))\n",
    "    model.add(Dense(3, name=\"dense_2\"))\n",
    "    ...\n",
    "    model.save_weights(fname)\n",
    "\"\"\"\n",
    "# new model\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=3, name=\"dense_1\")) # will be loaded\n",
    "model.add(Dense(10, name=\"new_dense\")) # will not be loaded\n",
    "\n",
    "# load weights from first model; will only affect the first layer, dense_1\n",
    "model.load_weights(fname, by_name=True)\n",
    "```\n",
    "\n",
    "## 为什么训练误差比测试误差高很多？\n",
    "\n",
    "一个Keras的模型有两个模式：训练模式和测试模式。一些正则机制，如Dropout，L1/L2正则项在测试模式下将不被启用。\n",
    "\n",
    "另外，训练误差是训练数据每个batch的误差的平均。在训练过程中，每个epoch起始时的batch的误差要大一些，而后面的batch的误差要小一些。另一方面，每个epoch结束时计算的测试误差是由模型在epoch结束时的状态决定的，这时候的网络将产生较小的误差。\n",
    "\n",
    "\\[Tips\\] 可以通过定义回调函数将每个epoch的训练误差和测试误差作图展现，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与Keras无关。\n",
    "\n",
    "## 如何获取中间层的输出？\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
