{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe学习系列：其他常用层及参数\n",
    "\n",
    "本文讲解一些其他的常用层，包括：softmax_loss层，Inner Product层，accuracy层，reshape层和dropout层及其它们的参数配置。\n",
    "\n",
    "## 1、softmax-loss\n",
    "\n",
    "softmax-loss层和softmax层计算大致是相同的。softmax是一个分类器，计算的是类别的概率（Likelihood），是Logistic Regression的一种推广。Logistic Regression只能用于二分类，而softmax可以用于多分类。\n",
    "\n",
    "softmax与softmax-loss的区别：\n",
    "\n",
    "softmax计算公式：\n",
    "\n",
    "$p_{j}=\\cfrac{e_{j}^{o}}{\\sum_{k}e^{o_{k}}}$\n",
    "\n",
    "而softmax-loss计算公式：\n",
    "\n",
    "$L=-\\sum_{j}y_{j}\\log{p_{j}}$\n",
    "\n",
    "关于两者的区别更加具体的介绍，可参考：[softmax vs. softmax-loss](http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/)。\n",
    "\n",
    "用户可能最终目的就是得到各个类别的概率似然值，这个时候就只需要一个Softmax层，而不一定要进行softmax-loss操作；或者是用户有通过其他什么方式已经得到了某种概率似然值，然后要做最大似然估计，此时则只需要后面的softmax-loss而不需要前面的softmax操作。因此提供两个不同的Layer结构比只提供一个合在一起的Softmax-Loss Layer要灵活许多。\n",
    "\n",
    "不管是softmax layer还是softmax-loss layer，都是没有参数的，只是层类型不同而已。\n",
    "\n",
    "softmax-loss layer：输出loss值。\n",
    "\n",
    "```\n",
    "layer {\n",
    "    name: \"loss\"\n",
    "    type: \"SoftmaxWithLoss\"\n",
    "    bottom: \"ip1\"\n",
    "    bottom: \"label\"\n",
    "    top: \"loss\"\n",
    "}\n",
    "```\n",
    "\n",
    "softmax layer：输出似然值。\n",
    "\n",
    "```\n",
    "layer {\n",
    "    bottom: \"cls3_fc\"\n",
    "    top: \"prob\"\n",
    "    name: \"prob\"\n",
    "    type: \"Softmax\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 2、Inner Product\n",
    "\n",
    "全连接层，把输入当作成一个向量，输出也是一个简单向量（把输入数据blobs的width和height全变为1）。\n",
    "\n",
    "输入：$n \\times c_{0} \\times h \\times w$\n",
    "\n",
    "输出：$n \\times c_{1} \\times 1 \\times 1$\n",
    "\n",
    "全连接层实际上也是一种卷积层，只是它的卷积核大小和原数据大小一致。因此它的参数基本和卷积层的参数一样。\n",
    "\n",
    "层类型：InnerProduct\n",
    "\n",
    "param的设置：<br>\n",
    "&emsp;&emsp;lr_mult：学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult，则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。\n",
    "\n",
    "必须设置的参数：<br>\n",
    "&emsp;&emsp;num_output：过滤器（filter）的个数\n",
    "\n",
    "其他参数：<br>\n",
    "&emsp;&emsp;weight_filter：权值初始化。默认为\"constant\"，值全为0，很多时候我们用\"xavier\"算法来进行初始化，也可以设置为\"gaussian\"<br>\n",
    "&emsp;&emsp;bias_filter：偏置项的初始化。一般设置为\"constant\"，值全为0<br>\n",
    "&emsp;&emsp;bias_term：是否开启偏置项，默认为true，开启\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"ip1\"\n",
    "    type: \"InnerProduct\"\n",
    "    bottom: \"pool2\"\n",
    "    top: \"ip1\"\n",
    "    param {\n",
    "        lr_mult: 1\n",
    "    }\n",
    "    param {\n",
    "        lr_mult: 2\n",
    "    }\n",
    "    inner_product_param {\n",
    "        num_output: 500\n",
    "        weight_filter {\n",
    "            type: \"xavier\"\n",
    "        }\n",
    "        bias_filter {\n",
    "            type: \"constant\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 3、accuracy\n",
    "\n",
    "输出分类（预测）精确度，只有test阶段才有，因此需要加入include参数。\n",
    "\n",
    "层类型：Accuracy\n",
    "\n",
    "示例：\n",
    "```\n",
    "name: \"accuracy\"\n",
    "type: \"Accuracy\"\n",
    "bottom: \"ip2\"\n",
    "bottom: \"label\"\n",
    "top: \"accuracy\"\n",
    "include {\n",
    "    phase: TEST\n",
    "}\n",
    "```\n",
    "\n",
    "## 4、reshape\n",
    "\n",
    "在不改变数据的情况下，改变输入的维度。\n",
    "\n",
    "层类型：Reshape\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"reshape\"\n",
    "    type: \"Reshape\"\n",
    "    bottom: \"input\"\n",
    "    top: \"output\"\n",
    "    reshape_param {\n",
    "        shape {\n",
    "            dim: 0  # copy the dimension from below\n",
    "            dim: 2\n",
    "            dim: 3\n",
    "            dim: -1 # infer it from the other dimensions\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "有一个可选的参数组shape，用于指定blob数据的各维的值（blob是一个四维的数据：$n \\times c \\times w \\times h$）。\n",
    "\n",
    "dim: 0——表示维度不变，即输入和输出是相同的维度。\n",
    "\n",
    "dim: 2或dim: 3——将原来的维度编程2或3。\n",
    "\n",
    "dim: -1——表示由系统自动计算维度。数据的总量不变，系统会根据blob数据的其他三维来自动计算当前维的维度值。\n",
    "\n",
    "假设原数据为：$64 \\times 3 \\times 28 \\times 28$，表示64张3通道的$28 \\times 28$的彩色图片。\n",
    "\n",
    "经过reshape变换：\n",
    "```\n",
    "reshape_param {\n",
    "    shape {\n",
    "        dim: 0\n",
    "        dim: 0\n",
    "        dim: 14\n",
    "        dim: -1\n",
    "    }\n",
    "}\n",
    "```\n",
    "输出数据为：$64 \\times 3 \\times 14 \\times 56$。\n",
    "\n",
    "## 5、Dropout\n",
    "\n",
    "Dropout是一个防止过拟合的trick。可以随机让网络某些隐含层节点的权重不工作。\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"drop7\"\n",
    "    type: \"Dropout\"\n",
    "    bottom: \"fc7-conv\"\n",
    "    top: \"fc7-dropout\"\n",
    "    dropout_param {\n",
    "        dropout_ratio: 0.5\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "只需要设置一个dropout_ratio就可以了。\n",
    "\n",
    "还有其他更多的层，但用的地方不多，就不一一介绍了。\n",
    "\n",
    "随着深度学习的深入，各种各样的新模型会不断的出现，因此对应的各种新类型的层也在不断的出现。这些新出现的层，我们只有在等caffe更新到新版本后，再去慢慢地摸索了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
