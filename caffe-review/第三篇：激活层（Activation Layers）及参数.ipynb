{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe学习系列：激活层（Activation Layers）及参数\n",
    "\n",
    "在激活层中，对输入数据进行激活操作（实际上就是一种函数变换），是逐元素进行运算的。从bottom得到一个blob数据输入，运算后，从top输出一个blob数据。在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的。\n",
    "\n",
    "输入：$n \\times c \\times h \\times w$<br>\n",
    "输出：$n \\times c \\times h \\times w$\n",
    "\n",
    "常用的激活函数有sigmoid，tanh，relu等，下面分别介绍。\n",
    "\n",
    "## 1、Sigmoid\n",
    "\n",
    "对每个输入数据，利用sigmoid函数执行操作。这种层设置比较简单，没有额外的参数。\n",
    "\n",
    "$S(x)=\\cfrac{1}{1+e^{-x}}$\n",
    "\n",
    "层类型：Sigmoid\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"encode1neuron\"\n",
    "    bottom: \"encode1\"\n",
    "    top: \"encode1neuron\"\n",
    "    type: \"Sigmoid\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 2、ReLU / Rectified-Linear and Leaky-ReLU\n",
    "\n",
    "ReLU是目前使用最多的激活函数，主要因为其收敛更快，并且能保持同样效果。\n",
    "\n",
    "标准的ReLU函数为max(x, 0)，当x＞0时，输出x；当x≤0时，输出0。\n",
    "\n",
    "$f(x)=max(x, 0)$\n",
    "\n",
    "层类型：ReLU\n",
    "\n",
    "可选参数：<br>\n",
    "&emsp;&emsp;negative_slope：默认为0。对标准的ReLU函数进行变化，如果设置了这个值，那么数据为负数时，就不再设置为0，而是用原始数据乘以negative_slope。\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"relu1\"\n",
    "    type: \"ReLU\"\n",
    "    bottom: \"pool1\"\n",
    "    top: \"relu1\"\n",
    "}\n",
    "```\n",
    "\n",
    "ReLU层支持in-place计算，这意味着bottom的输出和输入相同以避免内存的消耗。\n",
    "\n",
    "## 3、TanH / Hyperbolic Tangent\n",
    "\n",
    "利用双曲正切函数对数据进行变换。\n",
    "\n",
    "$tanh(x)=\\cfrac{sinh(x)}{cosh(x)}=\\cfrac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "层类型：TanH\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"layer\"\n",
    "    bottom: \"in\"\n",
    "    top: \"out\"\n",
    "    type: \"TanH\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 4、Absolute Value\n",
    "\n",
    "求每个输入数据的绝对值。\n",
    "\n",
    "$f(x)=Abs(x)$\n",
    "\n",
    "层类型：AbsVal\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"layer\"\n",
    "    bottom: \"in\"\n",
    "    top: \"out\"\n",
    "    type: \"AbsVal\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5、Power\n",
    "\n",
    "对每个输入数据进行幂运算。\n",
    "\n",
    "$f(x)=(shift + scale \\times x)^{power}$\n",
    "\n",
    "层类型：Power\n",
    "\n",
    "可选参数：<br>\n",
    "&emsp;&emsp;power：默认为1<br>\n",
    "&emsp;&emsp;scale：默认为1<br>\n",
    "&emsp;&emsp;shift：默认为0\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"layer\"\n",
    "    bottom: \"in\"\n",
    "    top: \"out\"\n",
    "    type: \"Power\"\n",
    "    power_param {\n",
    "        power: 2\n",
    "        scale: 1\n",
    "        shift: 0\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## 6、BNLL\n",
    "\n",
    "Binomial Normal Log Likelihood的简称。\n",
    "\n",
    "$f(x)=\\log(1+e^x)$\n",
    "\n",
    "层类型：BNLL\n",
    "\n",
    "示例：\n",
    "```\n",
    "layer {\n",
    "    name: \"layer\"\n",
    "    bottom: \"in\"\n",
    "    top: \"out\"\n",
    "    type: \"BNLL\"\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
